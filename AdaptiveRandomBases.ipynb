{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many prediction and regression schemes involve fitting models of the form \n",
    "$$\n",
    "\\hat f(x) = \\sum_{i=1}^N \\hat{a}(\\theta_i)\\phi(x;\\theta_i)\n",
    "$$\n",
    "\n",
    "where $\\phi(x;\\theta)$ is some function of $x\\in X$ depending on a parameter $\\theta\\in\\Theta$, and $\\hat a(\\theta_i)\\in \\R$. For example, neural networks and SVMs can both be expressed in this form. A natural way to view this strategy is as an approximation via a finite sum to an integral.\n",
    "\n",
    "$$\n",
    "f(x) = \\int_\\Theta a(\\theta) \\phi(x;\\theta)\\ d\\theta.\n",
    "$$\n",
    "\n",
    "The process of choosing values for $N$ and $\\theta_i$ vary from model to model. In Neural Networks, $N$ is fixed by the person constructing the model, while for an SVM $N$ is the number of support vectors. In Neural Networks, the $\\hat{a}(\\theta_i)$ and $\\theta_i$ are chosen by maximizing some (generally non-convex) objective function via gradient descent. In SVMs, the $\\theta_i$ are set by the data and the $a_i$ are chosen to maximize a (generally convex) objective function. In some instances (e.g. <a href=http://compneuro.uwaterloo.ca/research/nef.html>NEF</a>), the $\\theta_i$ are chosen <i>randomly</i> and the $a_i$ are then set by maximizing an objective function. The plan here is to set up a principled way to choose the $\\theta_i$ in a randomized fashion with accompanying error bounds.\n",
    "\n",
    "Choosing $\\theta_i$ randomly enjoys extremely fast optimization times since very little work is performed in picking the $\\theta_i$ and the optimization of the $\\hat{a}(\\theta_i)$ is now much easier (generally convex, and in the case of least-squares regression even has a closed-form solution). However it's not so clear that this randomization doesn't cost us a lot in terms of accuracy. To get a bound on how much accuracy we lose by choosing each $\\theta_i$ randomly, we can look at <a href=http://pages.cs.wisc.edu/~brecht/papers/08.Rah.Rec.Allerton.pdf>this paper</a> of Rahimi and Recht, which provides a nice bound. We'll briefly sketch their result. Suppose we choose each $\\theta_i$ independently from some distribution $p(\\theta)$. Let $\\|f\\|_p= \\sup_{\\theta}\\frac{|a(\\theta)|}{p(\\theta)}$. Suppose $\\|f\\|_p\\le \\infty$. Also suppose $|\\phi(x;\\theta)|\\le 1$ for all $x,\\theta$. Finally suppose we are interested in minimizing the expectation $\\E_x[(\\hat f(x) - f(x))^2]$ with respect to some given probability measure on the inputs $x$. Then with probability $1-\\delta$ there is a choice of $\\hat{a}(\\theta_i)$ such that\n",
    "$$\n",
    "\\E_x[(\\hat f(x) - f(x))^2] < \\frac{\\|f\\|_p}{\\sqrt{N}}\\left(1+\\sqrt{2\\log\\frac{1}{\\delta}}\\right).\n",
    "$$\n",
    "\n",
    "The proof follows from <a href=https://en.wikipedia.org/wiki/Doob_martingale#McDiarmid.27s_inequality>McDiarmid's inequality</a> and using $\\hat{a}(\\theta_i) = \\frac{a(\\theta_i)}{p(\\theta_i)N}$; effectively we are employing <a href=https://en.wikipedia.org/wiki/Importance_sampling>importance sampling</a> to approximate $f(x)$. \n",
    "\n",
    "This is a very interesting statement - as long as our sampling distribution $p(\\theta)$ is reasonably compatible with the function $f$, we will do pretty well by just randomly picking $\\theta_i$ - we can even pick them independently!\n",
    "\n",
    "This equation also implies that ideally we would sample $\\theta_i$ from some distribution $p(\\theta)$ such that $\\|f\\|_p$ is as small as possible. Thus a fully general approximation scheme would tune $p(\\theta)$ to perform better given data about the function of interest $f$. The optimal distribution from this perspective is one such that $p(\\theta)\\propto |a(\\theta)|$. However, it may not be very easy to acquire samples from this distribution. Instead we'll describe a simpler distribution, and then prove a bound on performance using this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before describing a proposal for $p(\\theta)$, let's examine a slightly different function, $\\tilde f(x)$. First, let $\\mu$ be a probability measure on inputs $x\\in X$. Then define an inner-product\n",
    "$$\n",
    "k(\\theta,\\theta') = \\int_X \\phi(x;\\theta)\\phi(x;\\theta')\\ \\mu(dx)\n",
    "$$\n",
    "\n",
    "Let\n",
    "$$\n",
    "c_\\theta = \\frac{\\int_\\Theta a(\\bar \\theta) k(\\theta,\\bar\\theta)\\ d \\bar\\theta}{\\int_\\Theta k(\\theta,\\bar \\theta)\\ d\\bar\\theta}\n",
    "$$\n",
    "\n",
    "so that $c_\\theta$ is the expectation of $a(\\bar\\theta)$ under the distribution $k_\\theta(\\bar\\theta)\\propto k(\\theta,\\bar\\theta)$. Then we set\n",
    "$$\n",
    "\\tilde f(x) = \\int_\\Theta c_\\theta \\phi(x;\\theta)\\ d\\theta.\n",
    "$$\n",
    "\n",
    "Our strategy will have two stages. First we will show that under some reasonable smoothness conditions, $\\tilde f(x)\\approx f(x)$. Then we will provide a way to sample from a $p(\\theta)$ with $p(\\theta)\\propto c_\\theta$. These results, together with the theorem of Rahimi and Recht will give a general way to approximate $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, one expects $\\tilde f$ to approximate $f$ because when the functions $\\phi(x;\\theta)$ are very different for different $\\theta$ values, we have $c_\\theta\\approx a(\\theta)$.\n",
    "\n",
    "In the following we formalize this intuition. To make life easier (at least in the long term hopefully), we'll define some more notation:\n",
    "$$\n",
    "K_\\theta = \\int_\\Theta k(\\theta,\\bar\\theta)\\ d\\bar\\theta\n",
    "$$\n",
    "$$\n",
    "k_\\theta(\\bar\\theta) = \\frac{k(\\theta,\\bar\\theta)}{K_\\theta}\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\int_\\Theta k_\\theta(\\bar\\theta)\\ d\\bar\\theta = 1\n",
    "$$\n",
    "$$\n",
    "c_\\theta = \\int_\\Theta a(\\bar \\theta) k_\\theta(\\bar\\theta)\\ d\\theta\n",
    "$$\n",
    "And we also make a couple of assumptions:\n",
    "\n",
    "<ol>\n",
    "<li>\n",
    "We'll quantify how fast $k_\\theta(\\bar\\theta)$ falls off (in particular, we assume that it does in fact fall off):\n",
    "$$\n",
    "\\int_\\Theta |\\theta-\\bar\\theta| k_\\theta(\\bar\\theta)\\ d\\bar\\theta = M_\\theta\n",
    "$$\n",
    "$$\n",
    "\\sup_\\Theta M_\\theta = M_\\sup\n",
    "$$\n",
    "</li>\n",
    "<li>\n",
    "Also, we will assume that $a(\\theta)$ is <a href=https://en.wikipedia.org/wiki/Lipschitz_continuity>Lipschitz continuous</a>:\n",
    "$$\n",
    "|a(\\theta)-a(\\theta')|\\le L_a |\\theta-\\theta'|\n",
    "$$\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "These two assumptions capture a few important pieces of intuition:\n",
    "<OL>\n",
    "<LI> We are assuming that the $\\theta$ values that are close together produce $\\phi(x;\\theta)$ functions that are similar (i.e. are correlated) and $\\theta$ values that are far apart produce $\\phi(x;\\theta)$ functions that are dissimilar.\n",
    "<LI> Since nearby $\\theta$ values produce similar basis functions $\\phi(x;\\theta)$, we should expect the weightings $a(\\theta)$ to be close to each other.\n",
    "</OL>\n",
    "\n",
    "We also define $|\\phi|^2$ as\n",
    "$$\n",
    "|\\phi|^2 = \\int_X\\left(\\int_\\Theta |\\phi(x;\\theta)|\\ d\\theta\\right)^2\\ \\mu(dx)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these assumption we can show a useful fact:\n",
    "\n",
    "Define $\\epsilon_\\theta =a(\\theta)-c_\\theta = \\int_\\Theta \\left(a(\\theta)-a(\\bar\\theta)\\right)k_\\theta(\\bar\\theta)\\ d\\bar\\theta$. We have\n",
    "$$\n",
    "|\\epsilon_\\theta | = \\left|\\int_\\Theta \\left(a(\\theta)-a(\\bar\\theta)\\right)k_\\theta(\\bar\\theta)\\ d\\bar\\theta\\right| \\le\\int_\\Theta |k_\\theta(\\bar\\theta)(\\theta-\\bar\\theta)L_a|\\ d\\bar\\theta \\le M_\\theta L_a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put this all together to see that:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\int_X (\\tilde f(x) - f(x))^2\\ \\mu(dx)&=\\int_X\\left(\\int_\\Theta (c_\\theta-a(\\theta)) \\phi(x;\\theta)\\ d\\theta\\right)^2\\ \\mu(dx)\\\\\n",
    "&\\le\\int_X \\left(\\int_\\Theta M_\\theta L_\\alpha |\\phi(x;\\theta)|\\ d\\theta\\right)^2 \\mu(dx)\\\\\n",
    "&\\le M_\\sup^2L_a^2|\\phi|^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this shows that when we use $c_\\theta$ in place of $a(\\theta)$ we might expect not to lose that much. However this doesn't immediately help - we don't actually know what $c_\\theta$ is either!\n",
    "\n",
    "Fortunately, $c_\\theta$ turns out to be much easier to estimate:\n",
    "$$\n",
    "\\begin{align}\n",
    "c_\\theta &= \\frac{\\int_\\Theta a(\\bar \\theta) k(\\theta,\\bar\\theta)\\ d \\bar\\theta}{\\int_\\Theta k(\\theta,\\bar \\theta)\\ d\\bar\\theta}\\\\ \n",
    "&= \\frac{\\int_\\Theta\\int_X \\phi(x;\\theta)\\phi(x;\\bar\\theta)a(\\bar\\theta)\\ \\mu(dx)\\, d\\bar\\theta}{\\int_\\Theta k(\\theta,\\bar \\theta)\\ d\\bar\\theta}\\\\\n",
    "&=\\frac{\\int_X \\phi(x;\\theta) f(x)\\ \\mu(dx)}{\\int_\\Theta k(\\theta,\\bar \\theta)\\ d\\bar\\theta}\\\\\n",
    "&=\\frac{\\E_X[ \\phi(x;\\theta) f(x)]}{\\E_X[\\int_\\Theta \\phi(x;\\theta)\\phi(x;\\bar\\theta)\\ d\\bar\\theta]}\n",
    "\\end{align}\n",
    "$$\n",
    "Given the ability to draw independent samples $(x,f(x))$ with $x$ distributed according to $\\mu$, the above allows us to estimate $c_\\theta$ using Monte-Carlo integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know how to estimate $c_\\theta$, we can sample from a distribution $p(\\theta)\\propto |c_\\theta|$ by various methods. One could use MCMC for this, but in the following we'll just compute $c_\\theta$ for a large number of $\\theta$ values and choose from the resulting discrete distribution over the chosen values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't feel like reading through the code (imagine that), I'll summarize the highlights here.\n",
    "\n",
    "In general we were looking for a function $\\hat f(x) = \\sum_{i=1}^N \\hat a(\\theta_i)\\phi(x;\\theta_i)$ that minimizes\n",
    "$$\n",
    "J = \\frac{1}{2}\\E_x[(\\hat f(x)-f(x))^2] + \\frac{\\lambda}{2}\\sum_{i=1}^N \\hat a(\\theta_i)^2\n",
    "$$\n",
    "\n",
    "I used two schemes:\n",
    "\n",
    "<ol>\n",
    "<li> randomly choosing $N$ fixed basis functions $\\phi(x;\\theta_1),\\dots,\\phi(x;\\theta_N)$ and optimizing weights $\\hat a(\\theta_i)$ in order to minimize $J$. Let's call this a \"fixed basis\" scheme.</li>\n",
    "<li> using the BFGS algorithm to optimize both $\\theta_i$ and $\\hat a(\\theta_i)$\n",
    "</ol>\n",
    "\n",
    "I set $\\phi(x;\\theta)=\\max(0,\\theta\\cdot x)$ (rectified linear units). The inputs $x$ were uniformly distributed in $[-1,1]^{10}$. To pick the function $f$, I chose $1000$ random $\\theta$ values (also uniformly distributed in $[-1,1]^{10}$ and computed the singular value decomposition of the matrix of values $\\phi(x_i,\\theta_j)$. $f$ was chosen to be the $10$th singular vector of this decomposition, scaled so that the average value of $f(x)^2$ was 1. The motivation behind this is that the singular vectors should represent in some sense a sequence of \"increasingly hard\" functions to approximate. This is because adding a little bit of $L_2$ regularization causes greater RMSE on vectors with lower singular values.\n",
    "\n",
    "Now for each of the two schemes above, I used \"non-adaptive\" and \"adaptive\" $\\theta$ values. For scheme 1 (fixed basis), \"non-adaptive\" means that the $\\theta_i$ are drawn from the same distribution that the $1000$ $\\theta$ values used to choose $f$ were drawn from. \"Adaptive\" means of course that they are chosen according to $c_\\theta$. For scheme 2, the chosen $\\theta$ values were used to initialize the BFGS algorithm.\n",
    "\n",
    "I computed (via trial-and-error basically; the code for this is not shown) values of $N$ for which the fixed-basis function schemes achieves a $J$ value of $0.0003125$ ($2.5\\%$ RMSE). The values are shown below:\n",
    "$$\n",
    "\\begin{array}{c|cc}\n",
    "N,\\text{ RMSE, time}&\\text{Fixed Basis Functions}&\\ \\text{BFGS}\\\\\n",
    "\\hline\n",
    "\\text{non-adaptive}&10000,\\ 0.026,\\ 22\\text{ seconds }&\\ 1000,\\ 0.033,\\ 1440\\text{ seconds }\\\\\n",
    "\\text{adaptive}&7000,\\ 0.025,\\ 24\\text{ seconds }&\\ 1000,\\ 0.033,\\ 1264\\text{ seconds }\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So one can see that using adaptive choices provides a significant reduction in number of basis functions required when we are not allowed to optimize the basis functions themselves. I didn't run the full BFGS optimization at a high enough number of basis vectors to achieve comporable error because I got too impatient. This highlights the point that the random methods are significantly faster than using a more complicated optimization proceedure. However, the BFGS optimization should be expected to achieve significant savings in terms of number of basis functions used even if it takes much longer to train.\n",
    "\n",
    "In fact, if we are satisfied with an RMSE of $0.05$ rather than $0.025$, then by optimizing the basis functions using BFGS we only need roughly $175$ basis functions while using the adaptively chosen random basis functions we need about $1200$, and $2000$ for the non-adaptive random method. As a further caveat, if we increase the difficulty of the problem by increasing the index of the singular value used to choose the function $f$ the number of basis functions needed to achieve low error increases very quickly, and the advantage accrued by using the adaptive method becomes less obvious. On the other hand, the cost in terms of time for doing the adaptive method is relatively small and so perhaps this is not such a big deal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok time for the code. Let's set up some framework for computing the output of a neural net with rectified linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <!-- collapse=True -->\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "from copy import copy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  6.99999873177e-06\n",
      "numerical to analytical difference:  5.81163073336e-07\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def reluprime(x):\n",
    "    return np.clip(np.sign(x),0,1)\n",
    "\n",
    "def EvalModel(X,E,W,tuning = relu):\n",
    "    return np.dot(W,tuning(np.dot(E,np.transpose(X))))\n",
    "\n",
    "def num_derivative(f,x,epsilon=0.000001):\n",
    "    fx = f(x)\n",
    "    xi = copy(x)\n",
    "    g = np.zeros(len(x))\n",
    "    for i in xrange(len(x)):\n",
    "        xi[i] += epsilon\n",
    "        g[i] = (f(xi)-f(x))/epsilon\n",
    "        xi[i] -= epsilon\n",
    "    return g\n",
    "\n",
    "def ReluDerivative(X,Z,E,W):\n",
    "    \"\"\"This function implements the chain rule (also called backprop)\"\"\"\n",
    "    Egrad = np.zeros(np.shape(E))\n",
    "    Wgrad = np.zeros(np.shape(W))\n",
    "    errs = EvalModel(X,E,W,relu)-Z\n",
    "    transdot = np.dot(E,np.transpose(X))\n",
    "    Wgrad = np.sum(errs*relu(transdot),axis = 1)\n",
    "    errfp = np.transpose(np.dot(np.transpose(X),W*np.transpose(errs*reluprime(transdot))))\n",
    "    return errfp/len(X),Wgrad/len(X)\n",
    "\n",
    "def ReluObjective(X,Z,E,W):\n",
    "    err = np.linalg.norm(EvalModel(X,E,W,relu)-Z)**2\n",
    "    return err*0.5/len(X)\n",
    "\n",
    "def RandEnc(n,d):\n",
    "    return np.random.uniform(-1,1,(n,d))\n",
    "\n",
    "def RunPackedRelu(x,P,n):\n",
    "    d = len(x)\n",
    "    E = np.reshape(P[:n*d],(n,d))\n",
    "    W = P[n*d:]\n",
    "    return EvalModel(x,E,W,relu)\n",
    "\n",
    "def PackParams(E,W):\n",
    "    return np.append(E.flatten(),W)\n",
    "\n",
    "def UnPackParams(P,n,d):\n",
    "    return np.reshape(P[:n*d],(n,d)),P[n*d:]\n",
    "\n",
    "def GetObjFunc(X,Z,n,d,l=0):\n",
    "    mask = np.zeros(n*d+n)\n",
    "    mask[n*d:]= 1.0\n",
    "    return lambda P: ReluObjective(X,Z,*UnPackParams(P,n,d))+l/2.0*np.linalg.norm(P*mask)**2\n",
    "\n",
    "def GetDerFunc(X,Z,n,d,l=0):\n",
    "    mask = np.zeros(n*d+n)\n",
    "    mask[n*d:]= 1.0\n",
    "    return lambda P: PackParams(*ReluDerivative(X,Z,*UnPackParams(P,n,d)))+l*P*mask\n",
    "\n",
    "def GetDecObjFunc(X,Z,E,n,d,l=0):\n",
    "    return lambda P: ReluObjective(X,Z,E,P)+l/2.0*np.linalg.norm(P)**2\n",
    "\n",
    "def GetDecDerFunc(X,Z,E,n,d,l=0):\n",
    "    return lambda P: ReluDerivative(X,Z,E,P)[1]+l*P\n",
    "\n",
    "def TestDer():\n",
    "    n = 10\n",
    "    d = 5\n",
    "    p = 3\n",
    "    E = RandEnc(n,d)\n",
    "    W = np.random.uniform(-1,1,n)\n",
    "    x = np.random.uniform(-1,1,(p,d))\n",
    "    z = [np.random.uniform(-1,1) for i in range(p)]\n",
    "    obj = GetObjFunc(x,z,n,d,0.2) #objective function\n",
    "    start = time.clock()\n",
    "    der = GetDerFunc(x,z,n,d,0.2) #derivative function\n",
    "    end = time.clock()\n",
    "    print \"Time: \",end-start\n",
    "    nd = num_derivative(obj,PackParams(E,W)) #numerical derivative\n",
    "    ad = der(PackParams(E,W)) #analytical derivative\n",
    "    \n",
    "    print \"numerical to analytical difference: \",np.linalg.norm(nd-ad)/np.linalg.norm(nd)\n",
    "    \n",
    "TestDer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective and derivative computation checks out. Now we'll setup a testing scenario. To do this we will first generate a random set of inputs and encoding weights and compute the SVD of the resulting activation function to get a number of functions with varying degrees of easiness to fit. The idea here is that singular vectors with low singular values are harder to fit - intuitively these are functions for which $a(\\theta)$ has high magnitude. Then we'll fit the functions with an standard black-box optimization algorithm (BFGS) as well as randomization and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <!-- collapse=False -->\n",
    "def randomX(p,d):\n",
    "    return np.random.uniform(-1,1,(p,d))\n",
    "\n",
    "def randomfuncs(p,d,n):\n",
    "    E = RandEnc(n,d)\n",
    "    X = randomX(p,d)\n",
    "    A = relu(np.dot(E,np.transpose(X)))\n",
    "    u,s,v = np.linalg.svd(A)\n",
    "    plt.plot(s)\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Singular Values')\n",
    "    plt.show()\n",
    "    return v*np.sqrt(p),E,X,s #rows of v are eigenvectors of AtA\n",
    "\n",
    "def fitfunc(X,Z,n,E0=None,l=0):\n",
    "    d = len(X[0])\n",
    "    p = len(Z)\n",
    "    grad = GetDerFunc(X,Z,n,d,l)\n",
    "    obj = GetObjFunc(X,Z,n,d,l)\n",
    "    if(E0==None):\n",
    "        E0 = RandEnc(n,d)\n",
    "    P0 = PackParams(E0,np.random.normal(0,0.1,n)) #initial guess\n",
    "    bounds = tuple([(-1,1) for i in xrange(n*d)]+[(None,None) for i in xrange(n)])\n",
    "    \n",
    "    opt = scipy.optimize.fmin_l_bfgs_b(func = obj,x0 = P0,fprime = grad,disp = -1, bounds = bounds)\n",
    "    xopt = opt[0]\n",
    "    fopt = obj(xopt)\n",
    "    return fopt\n",
    "\n",
    "\n",
    "def decode(X,Z,E,l=0.0):\n",
    "    \"\"\"computes a linear decode given inputs X,\n",
    "        targets Z, theta values in E, and regularization constant l\"\"\"\n",
    "    n = np.shape(E)[0]\n",
    "    d = np.shape(E)[1]\n",
    "    A = relu(np.dot(X,np.transpose(E)))\n",
    "    opt = scipy.sparse.linalg.lsqr(A,Z,np.sqrt(l*len(Z)/2))\n",
    "    obj = GetObjFunc(X,Z,n,d,l)\n",
    "    return obj(PackParams(E,opt[0]))\n",
    "\n",
    "def getdecode(X,Z,E,l=0.0):\n",
    "    n = np.shape(E)[0]\n",
    "    d = np.shape(E)[1]\n",
    "    A = relu(np.dot(X,np.transpose(E)))\n",
    "    opt = scipy.sparse.linalg.lsqr(A,Z,np.sqrt(l*len(Z)/2))\n",
    "    obj = GetObjFunc(X,Z,n,d,l)\n",
    "    return opt[0]\n",
    "\n",
    "def getlinearerror(X,Z,l=0.0):\n",
    "    \"\"\"computes the error from fitting Z with a linear map Z=WX\"\"\"\n",
    "    opt = scipy.sparse.linalg.lsqr(X,Z,np.sqrt(l*len(Z)/2))\n",
    "    return np.linalg.norm(np.dot(X,opt[0])-Z)**2/len(Z) + l/2.0*np.linalg.norm(opt[0])**2\n",
    "\n",
    "def Ctheta(theta,X,Z,tsamples = 1000,A=None,xsamples=None):\n",
    "    \"\"\"computes Ctheta distribution using tsamples\"\"\"\n",
    "    d = np.shape(X)[1]\n",
    "    p = len(Z)\n",
    "    if(xsamples == None):\n",
    "        xsamples = np.arange(len(X))\n",
    "    phitheta = relu(np.dot(X[xsamples],theta))\n",
    "    numerator = np.dot(phitheta,Z[xsamples])\n",
    "    \n",
    "    if(A == None):\n",
    "        Es = RandEnc(tsamples,d)\n",
    "        A = relu(np.dot(E,np.transpose(X))) #d by n\n",
    "\n",
    "    denominator =  np.sum(np.dot(A,phitheta)/samples)\n",
    "    \n",
    "    return numerator/denominator\n",
    "\n",
    "def getsamples(dist,N):\n",
    "    a = np.linalg.norm(dist,1)\n",
    "    p = np.abs(dist)/a\n",
    "    sampler = scipy.stats.rv_discrete(values=(range(len(dist)), p))\n",
    "    return sampler.rvs(size = N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEDCAYAAAAiKuN6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8ca85b2450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "d = 10\n",
    "p = 10000\n",
    "\n",
    "v,Ev,X,s = randomfuncs(p,d,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a list of functions stored in the matrix $v$. We'll use the $10^{th}$ singular vector to run our approximations. I've experimented around already to find numbers of basis functions to use that put the non-BFGS methods at an RMSE of nearly $0.025$. The function we're fitting will be normalized so that predicting zero at all points achieves an RMSE of $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Ctheta values\n",
      "Time:  4.480582\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "samples = 100000\n",
    "approxsamples = 100\n",
    "hardness = 10\n",
    "E = RandEnc(samples,d)\n",
    "\n",
    "\n",
    "start_theta = time.clock()\n",
    "xsamples = np.random.choice(np.arange(p),approxsamples)\n",
    "A = relu(np.dot(RandEnc(approxsamples,d),np.transpose(X[xsamples])))\n",
    "Cthetas = [Ctheta(theta,X,v[hardness],approxsamples,A=A,xsamples = xsamples) for theta in E]\n",
    "end_theta = time.clock()\n",
    "\n",
    "print \"Got Ctheta values\"\n",
    "print \"Time: \",end_theta-start_theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error:  0.119956800611\n",
      "Randomized Start decode error:  0.0261701919643\n",
      "Time:  22.2514216\n",
      "Adaptive Start decode error:  0.025090050377\n",
      "Time:  19.2620488\n",
      "Total Adaptive Time:  23.7426308\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "numada = 7000\n",
    "numrand = 10000\n",
    "l=0.1\n",
    "print \"Baseline Error: \",np.sqrt(2*getlinearerror(X,v[hardness],0))\n",
    "\n",
    "rtime = 0\n",
    "ropt = 0\n",
    "numi = 10\n",
    "for i in range(numi):\n",
    "    indices = getsamples(np.ones(len(E)),numrand)\n",
    "    E0 = RandEnc(numrand,d)#E[indices]\n",
    "    start = time.clock()\n",
    "    opt = decode(X,v[hardness],E0,l)\n",
    "    end= time.clock()\n",
    "    rtime += (end-start)/float(numi)\n",
    "    ropt += opt/numi\n",
    "print \"Randomized Start decode error: \",np.sqrt(2*ropt)\n",
    "print \"Time: \",rtime\n",
    "\n",
    "atime = 0\n",
    "aopt = 0\n",
    "for i in range(numi):\n",
    "    indices = getsamples(Cthetas,numada)\n",
    "    E0 = E[indices]\n",
    "    start = time.clock()\n",
    "    opt = decode(X,v[hardness],E0,l)\n",
    "    end= time.clock()\n",
    "    atime += (end-start)/float(numi)\n",
    "    aopt += opt/numi\n",
    "\n",
    "print \"Adaptive Start decode error: \",np.sqrt(2*aopt)\n",
    "print \"Time: \",atime\n",
    "\n",
    "print \"Total Adaptive Time: \",atime+end_theta-start_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Start BFGS Error:  0.0333144119113\n",
      "Time:  1440.8773858\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "numbfgs = 1000\n",
    "rbtime = 0\n",
    "rbopt = 0\n",
    "for i in range(numi):\n",
    "    indices = getsamples(np.ones(len(E)),numbfgs)\n",
    "    E0 = RandEnc(numbfgs,d)#E[indices]\n",
    "    start = time.clock()\n",
    "    opt = fitfunc(X,v[hardness],numbfgs,E0=E0,l=l)\n",
    "    end = time.clock()\n",
    "    rbtime += (end-start)/numi\n",
    "    rbopt += opt/numi\n",
    "print \"Random Start BFGS Error: \",np.sqrt(2*rbopt)\n",
    "print \"Time: \",rbtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Start BFGS Error:  0.0339679040565\n",
      "Time:  1260.1757173\n",
      "Total Adaptive BFGS Time:  1264.6562993\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "numbfgs = 1000\n",
    "abtime = 0\n",
    "abopt = 0\n",
    "for i in range(numi):\n",
    "    indices = getsamples(Cthetas,numbfgs)\n",
    "    E0 = E[indices]\n",
    "    start = time.clock()\n",
    "    opt = fitfunc(X,v[hardness],numbfgs,E0=E0,l=l)\n",
    "    end = time.clock()\n",
    "    abtime += (end-start)/numi\n",
    "    abopt += opt/numi\n",
    "print \"Adaptive Start BFGS Error: \",np.sqrt(2*abopt)\n",
    "print \"Time: \",abtime\n",
    "print \"Total Adaptive BFGS Time: \",abtime +end_theta-start_theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
