{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Expectation-Maximization algorithm (EM algorithm) is a an algorithm that anyone learning about statistics or ML will probably hear about eventually. It is used to fit a parameterized distribution to observed data. However the underlying workings of it (at least to me) seem far more convoluted than a method such as gradient descent on the likelihood function. So we're going to go through a careful investigation of how it works.\n",
    "\n",
    "To start with we need to set up the problem: Suppose we are given a bunch of data $X$ and wish build a model of the distribution that generates this data. To do this we'll suppose the distribution belongs to a family of distributions $P(X;\\theta)$ where $\\theta$ is some parameter. We'll also including in our model some <i>latent</i> or unobserved data, $Z$. Thus our full parametrized model is $P(X,Z;\\theta)$. $Z$ represents some additional structure in the data that is not provided by $X$ but may be important in generating $X$. For example, if $X$ is a set of audio files of different birds, $Z$ could be the species of bird that produced each file. \n",
    "\n",
    "Associated to any $X$ and $Z$ we define the likelihood function:\n",
    "$$\n",
    "L(\\theta;X,Z)  = P(X,Z;\\theta)\n",
    "$$\n",
    "and given any $X$ the marginal likelihood is given by\n",
    "$$\n",
    "L(\\theta;X)=\\sum_Z P(X,Z;\\theta)\n",
    "$$\n",
    "which is just the probability of observing the data $X$ marginalized over all possible $Z$ values. We want to find the <a href=https://en.wikipedia.org/wiki/Maximum_likelihood>maximum likelihood estimate</a>, which is just a way of saying that we believe the \"best\" value to choose for $\\theta$ is the one that is most likely to produce our observed data:\n",
    "$$\n",
    "\\theta^* = \\argmax_\\theta L(\\theta;X)\n",
    "$$\n",
    "\n",
    "Unfortunately, the likelihood function $L(X;\\theta)$ may not be so easy to compute - there may be an extremely large number of possible values for $Z$, in which case the sum in the previous equation becomes huge. Further, even when it is easy to compute the likelihood it may not be easy to compute the argmax to get $\\theta^*$. The point of EM is to provide a workaround for this issue. It doesn't work perfectly in that it is not guaranteed to provide the actual maximum likelihood estimate, but it may work well in practice.\n",
    "\n",
    "To be concrete, let's consider modeling human height. Male and female heights are approximately normally distributed, and each person has a $50\\%$ chance of being male or female. That is, heights can be fit with a <a href=https://en.wikipedia.org/wiki/Mixture_model><i>Gaussian mixture model</i></a>. In this scenario, we assume that our data is generated by a fixed number of Gaussian distributions (in our case, 2). So our parameters are $\\mu_0$, $\\mu_1$, $\\sigma_0$, $\\sigma_1$, $\\pi_0$, $\\pi_1$ where $\\mu_0$ is the average height of a male, $\\sigma_1$ is the variance in female heights, and $\\pi_0$ is the probability that any given person is a male. Of course we expect $\\pi_0=\\pi_1=0.5$, but for now let's pretend that we don't know that. Then our data $X=x_1,\\dots,x_N$ will be a set of observed human heights. The latent data $Z=z_1,\\dots,z_N$ is the gender of the observations ($z_i=1$ if the gender of the person with height $x_i$ is female, and 0 otherwise). \n",
    "\n",
    "Let's generate some simulated height data. Means and standard devaiations for male and female heights were pulled from <a href=www.wolframalpha.com/>Wolfram Alpha</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEPCAYAAABlZDIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8874954850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def GenHeights(N):\n",
    "    def getheight():\n",
    "        g = np.random.choice(['male','female'])\n",
    "        if(g=='male'):\n",
    "            return np.random.normal(167,22)\n",
    "        if(g=='female'):\n",
    "            return np.random.normal(156,17)\n",
    "    return np.array([getheight() for i in xrange(N)])\n",
    "\n",
    "X = GenHeights(1000000)\n",
    "plt.hist(X,100,normed = 1)\n",
    "plt.xlabel('Height (cm)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it's not really obvious from inspection that this is a mixture of two normal distributions, but nevertheless we'll hopefully be able to extract the relevant male and female height statistics.\n",
    "\n",
    "The EM algorithm is an iterative procedure; given a starting estimate of the parameters, $\\theta_0$, we will generate a series of estimates $\\theta_1,\\theta_2,\\dots,\\theta_t$ such that $L(\\theta_t;X)$ gets closer to the maximum likelihood $L(\\theta^*;X)$ as $t\\to\\infty$. With this in mind, suppose that we have some current estimate of the parameter, $\\theta_t$. We'll describe how to obtain $\\theta_{t+1}$.\n",
    "\n",
    "First define the function\n",
    "$$\n",
    "Q(\\theta|\\theta_t)=Q_t(\\theta) = \\E_{Z|X,\\theta_t}[\\log(L(\\theta;X,Z))] = \\sum_Z P(Z|X;\\theta_t)\\log(L(\\theta;X,Z))\n",
    "$$\n",
    "where \n",
    "$P(Z|X;\\theta_t)$ is the conditional probability of $Z$ given the data $X$ and the parameter $\\theta_t$. By Bayes' theorem:\n",
    "$$\n",
    "P(Z|X,\\theta_t) = \\frac{P(X,Z;\\theta_t)}{\\sum_{Z}P(X,Z;\\theta_t)}\n",
    "$$\n",
    "This is called the \"E\" or Expectation step. Most people use the $Q(\\theta|\\theta_t)$ notation. I found $Q_t(\\theta)$ was a little clearer for me when working through this so I'm going to use that for the remainder of this document.\n",
    "\n",
    "Then set\n",
    "$$\n",
    "\\theta_{t+1} = \\argmax_{\\theta} Q_t(\\theta)\n",
    "$$\n",
    "This is called the \"M\" or Maximization step. And that's it! Just repeat this over and over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this $Q_t$ function seems to have the same difficulty that computing the likelihood $L(\\theta;X)$ had - we have to compute a sum over $Z$. We're going to hope that for whatever examples we are interested in it's actually easy to compute $\\theta_{t+1}$ due to some nice structure our model that allows us to avoid computing the sum by brute force.\n",
    "\n",
    "Now this whole scheme may seem a bit arcane, so let's try to get a general sense of what's going on and why it's a good idea. Consider the following scheme: \n",
    "\n",
    "<ol>\n",
    "<li> Given $X,\\theta_t$, we get a distribution over possible $Z$'s - $P(Z|X;\\theta_t)$. Now let's actually pick a $Z$ from this distribution and assume that it is correct.</li>\n",
    "<li>Then given these values of $Z$, choose $\\theta_{t+1}$ to maximize the overall likelihood function $L(\\theta;X,Z)$. Since we've already picked $Z$ in a \"greedy\" manner, there is no need to marginalize over it to compute $L(\\theta;X)$ - in fact if our choice of $Z$ is correct, it is actually <i>better</i> not to marginalize over all $Z$s.</li>\n",
    "</ol>\n",
    "\n",
    "The EM algorithm is essentially performing an averaged version of the two steps above. Instead of actually committing to a greedy choice of $Z$, we average over all possible greedy choices of $Z$ and pick the $\\theta_t$ that will maximize this average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work though what all this boils down to in the Gaussian mixture height model. We have \n",
    "$$\n",
    "L(\\theta;X,Z)=P(X,Z;\\theta) = \\prod_{i=1}^N z_i \\frac{\\pi_1}{\\sigma_1\\sqrt{2\\pi} } \\exp\\left[ -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right] +(1-z_i)\\frac{\\pi_0}{\\sigma_0\\sqrt{2\\pi} } \\exp\\left[ -\\frac{(x-\\mu_0)^2}{2\\sigma_0^2} \\right]\n",
    "$$\n",
    "$$\n",
    "\\log(L(\\theta;X,Z)) = \\sum_{i=1}^N z_i\\left(\\log(\\pi_1)-\\log(\\sigma_1\\sqrt{2\\pi})-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right)+(1-z_i)\\left(\\log(\\pi_0)-\\log(\\sigma_0\\sqrt{2\\pi})-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "$$\n",
    "$$\n",
    "P(z_i=j|X,\\theta_t) =  \\frac{\\pi_j^{(t)} \\ p(x_i;\\mu_j^{(t)},\\sigma_j^{(t)})}{\\pi_1^{(t)} \\ p(x_i;\\mu_1^{(t)},\\sigma_1^{(t)}) + \\pi_0^{(t)} \\ p(x_i;\\mu_0^{(t)},\\sigma_0^{(t)})} \n",
    "$$\n",
    "where \n",
    "$$\n",
    "p(x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]\n",
    "$$\n",
    "is the Gaussian density function and we are using $\\theta_t = (\\pi_1^{(t)},\\mu_1^{(t)},\\sigma_1^{(t)},\\pi_2^{(t)},\\dots)$.\n",
    "Putting this together we have\n",
    "$$\n",
    "Q_t(\\theta) = \\sum_{i=1}^N p(z_i=1|X,\\theta_t)\\left(\\log(\\pi_1)-\\log(\\sigma_1\\sqrt{2\\pi})-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right)+p(z_i=0|X,\\theta_t)\\left(\\log(\\pi_0)-\\log(\\sigma_0\\sqrt{2\\pi})-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that since we assume $z_i$ and $z_j$ are independent, $Q_t(\\theta)$ is actually only a linear sum; if our model were more complicated it might become a very difficult exponential sum. It is not actually necessary to compute $Q_t(\\theta)$ in this case. For the M step, we only want to maximize it. So let's do that. Note that the $\\pi_j$s appear only in the $\\log(\\pi_j)$ term and so can be maximized seperately. We have $\\sum_j \\pi_j=1$, so using Lagrange multipliers:\n",
    "$$\n",
    "\\frac{d Q_t(\\theta)}{d \\pi_j} = \\frac{1}{\\pi_j}\\sum_{i=1}^Np(z_i=j|X,\\theta_t)=\\lambda\n",
    "$$\n",
    "$$\n",
    "\\pi_j = \\lambda \\sum_{i=1}^Np(z_i=j|X,\\theta_t)=\\frac{1}{N}\\sum_{i=1}^Np(z_i=j|X,\\theta_t)\n",
    "$$\n",
    "\n",
    "Further, we can maximize $\\mu_0,\\ \\sigma_0$ independently of $\\mu_1,\\ \\sigma_1$. So let's do that:\n",
    "$$\n",
    "\\frac{d Q_t(\\theta)}{d\\mu_0} = \\sum_{i=1}^N p(z_i=0|X,\\theta_t) \\left(-\\frac{x_i-\\mu_0}{\\sigma_0^2}\\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{d Q_t(\\theta)}{d\\sigma_0} = \\sum_{i=1}^N p(z_i=0|X,\\theta_t) \\left(\\frac{(x_i-\\mu_0)^2}{\\sigma_0^3}-\\frac{1}{\\sigma_0}\\right)\n",
    "$$\n",
    "From the first equation we have\n",
    "$$\n",
    "\\mu_0=\\frac{\\sum_{i=1}^N p(z_i=0|X,\\theta_t) x_i}{\\sum_{i=1}^N p(z_i=0|X,\\theta_t)}\n",
    "$$\n",
    "and so from the second we have\n",
    "$$\n",
    "\\sigma_0 = \\sqrt{\\frac{\\sum_{i=1}^N p(z_i=0|X,\\theta_t)(x_i-\\mu_0)^2}{\\sum_{i=1}^Np(z_i=0|X,\\theta_t)}}\n",
    "$$\n",
    "$\\mu_1$ and $\\sigma_1$ have similar updates from symmetry. \n",
    "\n",
    "For another (essentially similar) working through of the Gaussian mixture EM steps you can check out the <a href=https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Gaussian_mixture>example in the wikipedia article</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, these updates are actually very reasonable. $\\pi_j$ is updated to be an estimate of how often $z_i=j$. $\\mu_j$ is updated to an estimate of the mean of all the $x_i$s for which $z_i=j$, and $\\sigma_j$ is updated to be the standard deviation of the same. Anyway, here it is in code. I'm going to try to spell out a lot of the variable assignments in order to make it clearer what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male statistics: (learned, actual)\n",
      "Probability of being male: 0.54, 0.50\n",
      "Average Height: 166.6, 167.0\n",
      "Height Std Dev: 22.0, 22.0\n",
      "\n",
      "Female statistics: (learned, actual)\n",
      "Probability of being female: 0.46, 0.50\n",
      "Average Height: 155.6, 156.0\n",
      "Height Std Dev: 16.7, 17.0\n"
     ]
    }
   ],
   "source": [
    "# <!-- collapse=False -->\n",
    "import scipy.stats\n",
    "def pdf(x,mu,sigma):\n",
    "    \"\"\"Gaussian pdf with mean x and standard deviation sigma\"\"\"\n",
    "    return scipy.stats.norm(loc=mu,scale=sigma).pdf(x)\n",
    "\n",
    "def Pz(X,pi0,pi1,mu0,mu1,s0,s1):\n",
    "    \"\"\"Returns a matrix of value for p(z_i=j|X,\\theta_t). \n",
    "       Assumes X is an array containing all of the data\"\"\"\n",
    "    p0 = pi0*pdf(X,mu0,s0)\n",
    "    p1 = pi1*pdf(X,mu1,s1)\n",
    "    pnorm = p0+p1\n",
    "    assert(np.shape( np.array([p0/pnorm,p1/pnorm])) == (2,len(X)))\n",
    "    return np.array([p0/pnorm,p1/pnorm])\n",
    "\n",
    "    \n",
    "def Update(X,_pi0,_pi1,_mu0,_mu1,_s0,_s1):\n",
    "    \"\"\"EM update step\"\"\"\n",
    "    P = Pz(X,_pi0,_pi1,_mu0,_mu1,_s0,_s1)\n",
    "    pi0 = np.average(P[0])\n",
    "    pi1 = np.average(P[1])\n",
    "    \n",
    "    mu = np.dot(P,X)/np.sum(P,axis=1)\n",
    "    mu0 = mu[0]\n",
    "    mu1 = mu[1]\n",
    "    \n",
    "    sigma0 = np.sqrt(np.sum(P[0]*(X-mu0)**2)/np.sum(P,axis = 1)[0])\n",
    "    sigma1 = np.sqrt(np.sum(P[1]*(X-mu1)**2)/np.sum(P,axis = 1)[1])\n",
    "    \n",
    "    return pi0,pi1,mu0,mu1,sigma0,sigma1\n",
    "\n",
    "def RunEM(X,numiter,disp=0):\n",
    "    \"\"\"Run EM algorithm for mixture of two Gaussians for numiter iterations, \n",
    "       printing output every disp iterations.\n",
    "       Initial values are chosen randomly. \n",
    "       There is no rationale for the variances in the initialization; \n",
    "       I basically just picked some numbers at random.\"\"\"\n",
    "    mu0= np.random.normal(np.average(X),10)\n",
    "    mu1= np.random.normal(np.average(X),10)\n",
    "    s0 = np.sqrt(np.average(X**2)-np.average(X)**2)\n",
    "    s1 = np.random.normal(s0,3)\n",
    "    pi0 = np.random.uniform(0,1)\n",
    "    pi1 = 1- pi0\n",
    "    for i in xrange(numiter):\n",
    "        pi0,pi1,mu0,mu1,s0,s1 = Update(X,pi0,pi1,mu0,mu1,s0,s1)\n",
    "        if(disp>0):\n",
    "            if(i%disp ==0):\n",
    "                print \"On iteration \"+str(i)+\": theta=\"+str((pi0,pi1,mu0,mu1,s0,s1))\n",
    "    return pi0,pi1,mu0,mu1,s0,s1\n",
    "    \n",
    "\n",
    "pi0,pi1,mu0,mu1,s0,s1 = RunEM(X,1000)\n",
    "if(mu0>mu1): #Let's assume that the average male is taller than the average female\n",
    "    (pim,pif,mum,muf,sm,sf) = (pi0,pi1,mu0,mu1,s0,s1)\n",
    "else:\n",
    "    (pim,pif,mum,muf,sm,sf) = (pi1,pi0,mu1,mu0,s1,s0)\n",
    "    \n",
    "print \"Male statistics: (learned, actual)\"\n",
    "print \"Probability of being male: %.2f, %.2f\" %(pim,0.5)\n",
    "print \"Average Height: %.1f, %.1f\" %(mum,167)\n",
    "print \"Height Std Dev: %.1f, %.1f\" %(sm,22)\n",
    "\n",
    "print \"\\nFemale statistics: (learned, actual)\"\n",
    "print \"Probability of being female: %.2f, %.2f\" %(pif,0.5)\n",
    "print \"Average Height: %.1f, %.1f\" %(muf,156)\n",
    "print \"Height Std Dev: %.1f, %.1f\" %(sf,17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that worked pretty well! To finish up, let's prove something about why we should expect the EM algorithm to work.\n",
    "\n",
    "Now since the likelihood $L(\\theta;X)$ is generally a non-convex function we probably can't expect any guarantee that we'll converge to the actual maximum (indeed if you run the Height data multiple times, occasionally it gets caught in a local minimum in which it tries to fit the data with a single Gaussian by setting $pi_j=0$ for some $j$). So instead what we will show is that $L(\\theta;X)$ increases at every iteration of the algorithm.\n",
    "\n",
    "As a first step to get intuition, let's consider the case in which the EM algorithm can't go any further - that is when $\\theta_{t+1}=\\argmax_{\\theta} Q_t(\\theta) = \\theta_t$. In this case we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta Q_t(\\theta_t) &= 0\\\\\n",
    "\\sum_{Z} \\frac{p(X,Z;\\theta_t)}{p(X;\\theta_t)}\\frac{\\nabla_\\theta P(X,Z|\\theta_t)}{p(X,Z;\\theta_t)}&=0\\\\\n",
    "\\frac{1}{p(X;\\theta_t)}\\sum_{Z} \\nabla_\\theta p(X,Z;\\theta_t)&=0\\\\\n",
    "\\nabla_\\theta L(\\theta_t;X) &=0\n",
    "\\end{align*}\n",
    "$$\n",
    "so that any local optimum for the EM algorithm is necessarily a local optimum of the overall likelihood.\n",
    "\n",
    "This suggests (but does not prove yet!) that the sequence of $\\theta_t$s produced by the EM algorithm should have a sequence of likelihoods $L(\\theta_t;X)$ that converge to a local maximum of $L(\\theta;X)$. Let's go one step further and show that the EM algorithm will always increase the likelihood at every iteration\n",
    "For any given $Z$, we have\n",
    "$$\n",
    "p(X;\\theta)p(Z;X,\\theta) = p(X,Z;\\theta)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log L(\\theta;X)=\\log(p(X;\\theta)) &= \\log(p(X,Z;\\theta)-\\log(p(Z;X,\\theta))\\\\\n",
    "&=\\log L(\\theta;X,Z) -\\log(p(Z|X;\\theta))\n",
    "\\end{align*}\n",
    "$$\n",
    "Now taking expectations over $Z$ given $X$ and $\\theta_t$ should not change the left-hand side since it is a constant. Therefore:\n",
    "$$\n",
    "\\log L(\\theta;X) = Q_t(\\theta) - \\E_{Z|X,\\theta_t}\\log(p(Z;X,\\theta))\n",
    "$$\n",
    "Then we have\n",
    "$$\n",
    "\\log L(\\theta_{t+1};X)-\\log L(\\theta_t;X) = Q_t(\\theta_{t+1})-Q_t(\\theta_t)\\\\ \n",
    "+\\E_{Z|X,\\theta_t}\\log(p(Z|X;\\theta_{t}))- \\E_{Z|X;\\theta_t}\\log(p(Z|X,\\theta_{t+1}))\\\\\n",
    "=\\left(Q_t(\\theta_{t+1})-Q_t(\\theta_t)\\right) +\\E_{Z|X,\\theta_t}\\left[\\log\\frac{p(Z|X;\\theta_{t})}{p(Z|X,\\theta_{t+1})}\\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference on the right hand side is $\\ge 0$ by definition of $\\theta_{t+1}$ ($\\theta_{t+1}$ maximizes $Q_t(\\theta)$). The last term is actually the <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>KL divergence</a> between $p(Z|X;\\theta_t)$ and $p(Z|X;\\theta_{t+1})$ and so must also be $\\ge 0$. Together this implies that $L(\\theta_{t+1};X)\\ge L(\\theta_t;X)$ and so the EM algorithm constantly increases the likelihood. Just to be a little more comprehensive, we'll lay out the KL divergence argument here:\n",
    "\n",
    "Define the KL divergence between two distributions $P$ and $Q$ as\n",
    "$$\n",
    "KL(P\\|Q) = \\E_P[\\log(P/Q)] = \\sum_{X} P(X)\\log\\frac{P(X)}{Q(X)}\n",
    "$$\n",
    "Then via an application of <a href=https://en.wikipedia.org/wiki/Jensen%27s_inequality>Jensen's inequality</a> we have\n",
    "$$\n",
    "-KL(P\\|Q) = \\E_P[\\log(Q/P)]\\le \\log[\\E_P[Q/P]]=\\log(1)=0\n",
    "$$\n",
    "\n",
    "The EM algorithm generates a sequence of $\\theta_t$s such that $L(\\theta_t;X)$ is constantly increasing. Thus if we \n",
    "assume that there actually is a maximum likelihood estimate $\\theta_* = \\argmax_{\\theta} L(\\theta;X)$, then $L(\\theta_t;X)$ must converge to some value. Note that this is <i>NOT</i> the same as saying that $\\theta_t$ converges. For more on when you can expect $\\theta_t$ itself to converge, see Jeff Wu's paper \"On the convergence properties of the EM algorithm\". A pdf is <a href=http://artishoke.com/papers/ML/EMconvergence.pdf>here</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
