{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Overfitting and Generalization Bounds</h1>\n",
    "\n",
    "Suppose you want to write a program that predicts how much rain there will be tomorrow from 1000 different types of weather data recorded today (average wind-speed, barometric pressure, percent cloud cover, temperature, etc\n",
    "). You aren't a meteorologist though, so you don't know anything about how the rain should actually be influenced by these variables. Instead, you're going to apply this new-fangled machine learning stuff that you've heard so much about. You remember from physics class that all objects are point masses and all functions are linear, so you decide that the amount of precipitation probably can be written as\n",
    "$$\n",
    "r = h[0]x[0]+h[1]x[1]+\\cdots+h[999] x[999]\n",
    "$$\n",
    "where here $r$ is the amount of precipitation (say in inches), $x[0],\\dots,x[999]$ are your weather measurements, and $h[0],\\dots,h[999]$ are some constants that you don't know. Let's compactify our notation a bit. We'll use vector notation and write $h=(h[0],\\dots,h[999])$ and $x=(x[0],\\dots,x[999])$ so that\n",
    "$$\n",
    "r=h\\cdot x\n",
    "$$\n",
    "Now you have to do is find the values of $h[i]$, and then you're done!\n",
    "\n",
    "In order to do this, you record weather data for 1000 days (maybe you did this in 50 places for 20 days each, or maybe you spent 3 years, whatever floats your boat), and use a nice easy optimization package to find values for $a_i$ that predict your recorded data well. You find that you can predict your collected data perfectly! This is extremely exciting, but before rushing to start a weather-prediction company, you decide to check if your predictions hold up. So you collect data for 2000 more days, and check how well your predictions match up with reality. This time, you discover to your chagrin that your predictions are horrible! \n",
    "\n",
    "So what happened? The problem, of course, is <i>overfitting</i>. By minimizing your error on your training set (the 1000 days of training data), you adapted too strongly to the peculiarities of that dataset. Intuitively, your linear function is also predicting the noise that's present in your 1000 observations. It continued to try to add this noise in when making predictions on the 2000 new observations, and so ended up with very poor predictions because the noise is actually irrelevant. This intuition may also give you some idea about how to deal with this. One way is to simply collect more data to start with, so that it's easier for your linear function to identify the signal in the noise. But what if collecting data is expensive and you only have money for 1000 training measurements? Then another approach would be to make the linear model <i>less powerful</i>. The problem here was that you were able to perfectly model the noise in your data. If you instead restricted yourself to only considering the first 100 measurements (for example), perhaps it would be impossible to model the noise properly and so you wouldn't overfit. On the other hand, it might be that by ignoring the last 900 observations you make it impossible to find a good predictor for the rain. This leads to a natural trade-off between the <i>capacity</i> of your model (the degree to which it could predict the rain if trained correctly), and the <i>generalization error</i> (the degree to which your model overfits to the training set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Some Mathematical Notation</h2>\n",
    "$\\newcommand{\\E}{\\mathop{\\mathbb{E}}}\\newcommand{\\L}{\\mathcal{L}}\\newcommand{\\H}{\\mathcal{H}}\\newcommand{\\R}{\\mathbb{R}}$\n",
    "Let's make our setting precise so that we can actually prove some things. In general, we are trying to find some function $h$ that \"performs well\" on an observed datapoint $z$. We'll measure the degree to which $h$ performs well with a loss function $\\L(h,z)$. More formally, let $Z$ be a space of possible observations (e.g. weather data and rain measurements), $\\H$ be a space of possible functions (e.g. our linear functions), and $\\L$ a function from $\\H\\times Z\\to \\R$. Let $D$ be some unknown distribution over $Z$. Then our objective is to find a function $h^\\star\\in \\H$ that minimizes $\\L(h)=\\E_{z\\sim D}[L(h,z)]$. The function $\\L$ is called the <i>risk</i>, and the set $\\H$ is often called a <i>hypothesis class</i>, while the functions $h$ are called hypotheses.\n",
    "\n",
    "In our weather example, $Z=X\\times \\R$ where $X$ is the space of tuples of measured weather data $(x[0],\\dots,x[999])$ and a data point $(x,r)$ indicates that $r$ inches of rain were observed the day after recording weather data $x$. $\\H$ is the set of linear functions $X\\to \\R$, and we might use the loss function $\\L(h,(x,r)) = \\left|h(x)-r\\right|$ (called the \"absolute loss\"). In this example, since each $h\\in \\H$ is specified by a vector $(h[0],\\dots,h[999])\\in \\R^{1000}$, we can think of $\\H$ as equal to $\\R^{1000}$.\n",
    "\n",
    "This problem of finding $h^\\star$ is made much more difficult by the fact that we don't know the distribution $D$. Instead, we have a training set, which is a sample of $N$ independent points $z_1,\\dots,z_N$ distributed according to $D$. How can we use this training set to find an $h$ with small $\\L(h)$? One natural approach is <i>empirical risk minimization</i>, which is just a fancy way of saying \"minimize your training error\". Formally, we define the <i>empirical risk</i> (a.k.a. training error) by $\\hat \\L(h)=\\frac{1}{N}\\sum_{i=1}^N \\L(h,z_i)$. Then we can hope that $\\hat \\L(h)$ is a good approximation for $\\L(h)$, and instead find $\\hat h$ that minimizes $\\hat \\L$, called the <i>empirical risk minimizer</i>. This is a pretty reasonable-sounding thing to do, since in general averages should converge to their expectated values. But we're not satisfied with just \"reasonable sounding things\". After all, this is exactly what we did in the weather example above. So in order to figure out how to do something better, let's try to analyze how well empirical risk minimization actually does at finding $h^\\star$.\n",
    "\n",
    "Specifically, we'll look for an equation of the form\n",
    "$$\n",
    "\\left|\\L(\\hat h)-\\hat \\L(\\hat h)\\right| \\le Q\n",
    "$$\n",
    "where $Q$ is a measure of how much we are overfitting. Thus if $Q$ is small, we can be confident that our training error $\\hat \\L(\\hat h)$ is a good approximation for our test error $\\L(\\hat h)$. You might point out that this doesn't really say anything about how well we are doing relative to the best point $h^\\star$. That is, wouldn't it be more useful to have a bound of the form:\n",
    "$$\n",
    "\\L(\\hat h)-\\L(h^\\star)\\le Q'\n",
    "$$\n",
    "Now $Q'$ tells us how much worse we are doing than the best predictor $h^\\star$. It turns out we'll actually get both kinds of bounds. Specifically, we'll end up with something like\n",
    "$$\n",
    "\\left|\\L(h)-\\hat \\L(h)\\right| \\le Q\n",
    "$$\n",
    "for all $h\\in \\H$. This type of bound implies $\\L(\\hat h)-\\L(h^\\star)\\le 2Q$. To see this, notice that $\\hat \\L(\\hat h)-\\hat \\L(h^\\star)\\le 0$ since $\\hat h$ minimizes $\\hat \\L$. Then we have:\n",
    "$$\n",
    "\\L(\\hat h)- \\L(h^\\star)=(\\L(\\hat h) - \\hat \\L(\\hat h)) + (\\hat \\L(\\hat h)-\\hat \\L(h^\\star)) + (\\hat \\L(h^\\star)  - \\L(h^\\star))\\le 2Q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Hoeffding's Bound</h2>\n",
    "So long as $\\L(h,z_i)$ isn't allowed to be too big (e.g. if $\\L(h,z_i)\\in[0,B]$ with probability 1 for some constant $B$), then <a href=https://en.wikipedia.org/wiki/Hoeffding_inequality>Hoeffding's inequality</a> tells us that for any $h\\in \\H$,\n",
    "$$\n",
    "\\Pr\\left[\\left| \\L(h)-\\hat \\L(h)\\right|>\\epsilon\\right]=\\Pr\\left[\\left| \\L(h)-\\frac{1}{N}\\sum_{i=1}^N \\L(h,z_i)\\right|>\\epsilon\\right] \\le 2\\exp\\left(\\frac{-2N\\epsilon^2}{B^2}\\right)\n",
    "$$\n",
    "This says that the average value, $\\frac{1}{N}\\sum_{i=1}^N \\L(h,z_i)=\\hat \\L(h)$ is almost certainly very very close to the expected value $\\L(h)$. In particular, the probability that the difference between the average and the expectation is greater than $\\frac{2B}{\\sqrt{2N}}$ is less than $0.04$. In general, for any $h\\in \\H$, with probability at least $1-\\delta$, we have\n",
    "$$\n",
    "\\left|\\L(h)-\\hat \\L(h)\\right| \\le  \\frac{B\\sqrt{\\log(2/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "This calculation makes it seem like $\\hat \\L$ is a great approximation for $\\L$, and so $\\hat h$ must be a really great approximation for the best function $h^\\star$. If we have $B=1$ (which in our setting corresponds to a loss function that is bounded between 0 and 1, like the 0-1 classification loss), then for $N=1000$ we should expect an error of less than $0.05$!\n",
    "\n",
    "This seems very promising, but unfortunately life is not so easy. If you're not already suspicious, here's an example illustrating that something is wrong:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"margin-left: 3em; margin-right: 6em;\" class=\"aside\">\n",
    "<p>\n",
    "Suppose you are fitting the weather-prediction problem again, but now you only care to predict whether or not there will be rain. You're going to again use a linear model $r = a_1x_1+a_2x_2+\\cdots+a_{1000} x_{1000}$ and output \"rain\" if $r>0$ and \"no rain\" if $r\\le 0$. The loss will be just be 1 if you predict correctly and 0 otherwise. This loss is bounded between 0 and 1, so $B=1$ and by the above argument you'd expect the empirical risk $\\hat L$ to be only 0.05 away from the true risk $\\L$. However, suppose that your 1000 weather measurements are really just noise - someone stole all your recording equipment and replaced them with random number generators. Then you should still expect to find an $\\hat h$ with $\\hat \\L(\\hat h)=0$ because you have a linear system with 1000 variables (the $a_i$'s) and 1000 equations (the 1000 training observations), so you can always find some $a_i$'s that will match the training points perfectly. However, since your actual measurements $x_i$ are garbage, it's silly to expect that the true best predictor would have expected loss of only 0.05.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So what went wrong? Essentially, we've run afoul of a multiple-hypothesis testing issue. Our Hoeffding bound equation holds individually for each $h\\in \\H$, but it does not hold for ALL values of $h\\in \\H$ <i>at the same time</i>. This can be kind of a tricky concept, so here's another example of what's happening.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<div style=\"margin-left: 3em; margin-right: 6em;\" class=\"aside\">\n",
    "<p>\n",
    "Suppose you and $1000$ friends each toss a biased coin that lands heads with probability $0.999$. Then you should be almost positive that your coin will land heads. However, it's <i>also</i> pretty likely that at least one of your friends' coins landed tails. Mathematically, the probability of none of your friends getting a toss of tails is $(0.999)^{1000}\\le 0.37$. Exactly the same thing is happening in our empirical risk minimization problem. The \"friends\" are the functions $h\\in \\H$. Each individual function $h$ has training error that's probably quite close to the true error - just as each individual coin is very strongly biased towards heads. However, the probability that there is <i>some</i> $h\\in \\H$ whose training error is very far from the true error is also very high. Since you are selecting the $h$ with the smallest training error, this will effectively bias you towards those $h$'s whose training errors have a big downward deviation from the true error, resulting in overfitting.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>The Union Bound</h2>\n",
    "How can we analyze this problem correctly? Well, the issue with the Hoeffding's inequality argument was that the probability bound holds only for each $h\\in H$ individually. Maybe we should instead look for a statement that holds for ALL $h\\in H$ simultaneously. This is called the \"uniform convergence\" approach, and is the basis for many generalization bounds. As a gentle foray into this area, let's assume that $\\H$ contains only 10 functions $h_1,\\dots,h_{10}$. Then for each function individually, we have\n",
    "$$\n",
    "\\Pr\\left[\\left| \\L(h_j)-\\hat \\L(h_j)\\right|>\\epsilon\\right] \\le 2\\exp\\left(\\frac{-2N\\epsilon^2}{B^2}\\right)\n",
    "$$\n",
    "Now we use the extremely useful <a href=https://en.wikipedia.org/wiki/union_bound>union bound</a>, which simply says that if there are $M$ \"bad\" events that occur with probabilities at most $p_1,\\dots,p_M$, then the probability that none of the bad events occur is at most $p_1+\\cdots+p_M$. Therefore the probability that none of the 10 functions in $\\H$ has an average loss that is far from the true loss is at most\n",
    "$$\n",
    "\\Pr\\left[\\max_{j\\in \\{1,\\dots,10\\}}\\left| \\L(h_j)-\\hat \\L(h_j)\\right|>\\epsilon\\right] \\le 10\\times2\\exp\\left(\\frac{-2N\\epsilon^2}{B^2}\\right)\n",
    "$$\n",
    "\n",
    "More generally, if $\\H$ contains $M$ functions, then\n",
    "$$\n",
    "\\Pr\\left[\\max_{h\\in \\H}\\left| \\L(h)-\\hat \\L(h)\\right|>\\epsilon\\right] \\le 2M\\exp\\left(\\frac{-2N\\epsilon^2}{B^2}\\right)\n",
    "$$\n",
    "\n",
    "If you set the right-hand-side of the above equation to $\\delta$ and solve for $\\epsilon$, then this tells us that with probability at least $1-\\delta$, we have\n",
    "$$\n",
    "\\left| \\L(h)-\\hat \\L(h)\\right| < \\frac{B\\sqrt{\\log(2M/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "For all $h\\in \\H$.\n",
    "\n",
    "Go back and compare this to the statement we got with just Hoeffding's inequality without the union bound. The only difference is the additional $M$ inside the logarithm. This seems pretty nice - we're only paying a logarithmic cost to pick a model out of $M$ possibilities. This sort of observation is at the root of multiple hypothesis testing. Each function $h\\in \\H$ corresponds to a different hypothesis, and the scientist is picking the hypothesis that best explains the data, which corresponds to having a low $\\hat L(h)$. Using this union bound identity allows the scientist to properly compute those $p$-values they are all so fond of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unfortunately, we still can't analyze our weather-prediction problem, because the space of linear functions is infinite! Even if the amount of overfitting only depends logarithmically on $M$, $\\log(\\infty)$ is still $\\infty$. So far we've use Hoeffding's bound and the union bound. We need a third ingredient to deal with infinitely large spaces $\\H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Covering Numbers</h2>\n",
    "One way to avoid the problem of not being able to deal with infinite $\\H$ is to simply never use an infinite $\\H$. For example, in our weather prediction problem we could pick some finite set of linear functions $h_1,\\dots,h_M$ before seeing any training data and just find the $h_i$ that has the smallest error on our training set. This would allow us to apply our Hoeffding+Union bound result to bound how much we overfit. Obviously we're losing something by restricting ourselves to only $M$ linear functions, but maybe if we picked a very large $M$, then this would be ok.\n",
    "\n",
    "This strategy isn't all that unreasonable because if you make a small change to the coefficients $h[i]$ of our linear function $h[0]x[0]+\\cdots + h[999]x[999]$, then the output shouldn't change that much. Specifically, if our weather data satisfies $\\sqrt{\\sum_{i=0}^{999} x[i]^2} \\le L$ for some constant $L$ and we change each $h[i]$ by some small amount $\\Delta[i]$, then by Cauchy-Schwarz inequality, the total value of $h\\cdot x = h[0]x[0]+\\cdots+h[999]x[999]$ will change by at most $L\\sqrt{\\sum_{i=0}^{999} \\Delta[i]^2}$. This means that if one of our finitely-many choices $h_i$ is \"close\" to the best choice $h^\\star$, then we'll have $\\L(h_i)-\\L(h^\\star)$ is very small, which means that we didn't lose much by selecting only a finite subset of $\\H$.\n",
    "\n",
    "How can we guarantee that $h^\\star$ will be close to one of our $h_i$? Maybe we can choose enough $h_i$ so that EVERY $h\\in \\H$ is close to some $h_i$. Unfortunately, this is actually impossible to do with finitely many $h_i$. However, if we restrict $\\H$ to be only contain bounded linear functions, then we're back in business. Specifically, let's say that $\\H_R$ is the set of linear functions given by vectors $h=(h[0],\\dots,h[999])\\in \\R^{1000}$ such that the $\\|h\\|\\le R$. That is, $\\sqrt{\\sum_{i=0}^{999} h[i]^2}\\le R$. Notice that $\\H_R$ is still infinite so this restriction doesn't immediately give us a generalization bound.\n",
    "\n",
    "Now our problem of finding $M$ functions $h_1,\\dots,h_M$ that every point in $\\H_R$ is close to one of our $h_i$ is the same as finding $M$ points in $\\R^{1000}$ such that every point in the ball of radius $R$ about the origin in $\\R^{1000}$ is close to one of our $M$ points.\n",
    "\n",
    "Suppose that we can pick $M$ points $h_1,\\dots,h_M\\in \\R^{1000}$ such that $\\|h-h_i\\|\\le \\Delta$ for all $h\\in \\R^{1000}$ with $\\|h\\|\\le R$. If we pick the point $\\hat h_i$ that minimizes $\\hat \\L(h_i)$, then by our Hoeffding inequality and Union bound argument we have\n",
    "$$\n",
    "\\left|\\L(\\hat h_i) -\\hat \\L(h_i)\\right|\\le \\frac{B\\sqrt{\\log(2M/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "for all $i\\in\\{1,\\dots,M\\}$.\n",
    "Now let's further suppose that we are guaranteed $\\|x\\|=\\sqrt{\\sum_{i=1}^{1000} x_i^2} \\le L$, then $\\left|\\L(h^\\star)-\\L(h_i)\\right|\\le L\\|h^\\star-h_i\\|$ for all $i$. Since for all $h\\in \\H$ there is some $h_i$ with $\\|h-h_i\\|\\le \\Delta$, this means that by our Cauchy-Schwarz argument, for this $h_i$ we have $\\left|\\L(h)-\\L(h_i)\\right|\\le \\Delta L$ and $\\left|\\hat \\L(h)-\\hat \\L(h_i)\\right|\\le \\Delta L$. Therefore for all $h\\in \\H_R$,\n",
    "$$\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|\\le  2 L\\Delta + \\frac{B\\sqrt{\\log(2M/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "Notice that $\\hat h_i$ doesn't show up in this equation at all! The only sign of the finite set of $h_i$ is the number $M$, which is the size of the finite set.\n",
    "\n",
    "We call this number $M$ a <i>covering number</i>. Specifically, we write $M=N(\\Delta,\\H_R)$ to mean that $M$ is the size of the smallest set of points $h_1,\\dots,h_M$ such that every $h\\in \\H_R$ is within $\\Delta$ of some $h_i$.\n",
    "\n",
    "I'm not going to go into this argument here, but with a bit of trickery with volumes of spheres, it is possible to show that\n",
    "$$\n",
    "N(\\Delta,\\H_R)\\le \\left(1+\\frac{2R}{\\Delta}\\right)^{1000}\n",
    "$$\n",
    "where the exponent $1000$ comes from the fact that $\\H_R$ is a radius $R$-ball in $\\R^{1000}$. More generally in $\\R^d$ the covering number is bounded by $\\left(1+\\frac{2R}{\\Delta}\\right)^d$.\n",
    "\n",
    "Now let's plug this expression into our generalization bound, using $d=1000$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|&\\le  2 L\\Delta + \\frac{B\\sqrt{\\log(2N(\\Delta, \\H_R)/\\delta)}}{\\sqrt{2N}}\\\\\n",
    "&\\le  2 L\\Delta + \\frac{B\\sqrt{d\\log\\left(1+\\frac{2R}{\\Delta}\\right)+\\log(2/\\delta)}}{\\sqrt{2N}}\n",
    "\\end{align*}\n",
    "$$\n",
    "Here's a final twist - the value of $\\Delta$ is completely arbitrary! The above equation holds (with probability $1-\\delta$) regardless of what $\\Delta$ is, so we are free to pick it however we like! Let's try the value $\\Delta = \\frac{R}{\\sqrt{N}}$. Then we have (with probability $1-\\delta$, for all $h\\in \\H_R$):\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|&\\le\\frac{2\\sqrt{2}LR + B\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(2/\\delta)}}{\\sqrt{2N}}\\\\\n",
    "&=O\\left(\\frac{LR + B\\sqrt{d\\log(N)+\\log(1/\\delta)}}{\\sqrt{N}}\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Getting Rid Of $\\H_R$</h2>\n",
    "\n",
    "Now we've used three ingredients (Hoeffding's inequality, Union bound, and covering numbers) to get a bound on the generalization performance when we restrict ourselves to $\\H_R$. But this is still a bit of a cop-out since our original problem didn't have any restrictions. How can we move from $\\H_R$ to $\\H$?\n",
    "\n",
    "To do this we're going to have to be a bit more subtle. So far we've found bounds like\n",
    "$$\n",
    "\\left| \\hat \\L(h)-\\L(h)\\right| \\le Q\n",
    "$$\n",
    "for some <i>constant</i> value $Q$. To move to $\\H$, we're going to instead find a bound like\n",
    "$$\n",
    "\\left| \\hat \\L(h)-\\L(h)\\right| \\le Q(h)\n",
    "$$\n",
    "so that now our measure of generalization performance is a function of the point we are measuring at. In a bit we'll argue that this sort of measure is actually <i>better</i> than just using a constant value because it will actually allow us to do better than empirical risk minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tackle this problem, let's first consider which variables in our $\\H_R$ bound depend on the restriction to $\\H_R$. There are 2. The most obvious one is $R$, but a more subtle one I snuck under the rug in the last section is $B$, the bound on $\\L(h,z_i)$. Recall that we write $z=(x,r)$ to indicate that $z$ contains both the measured weather data $x$ as well as the actual amount of rain $r$. If we use the \"absolute loss\" $\\L(h,(x,r)) = \\left|h(x)-r\\right|$, then the maximum value of $\\L(h,(x,r))$ gets bigger as $D$ increases. In particular, let's assume that $r$ is always at most $R$ for some value $R$ (this is just the maximum amount of possible rain in a day). Then if $\\|x\\|\\le L$, we have $\\L(h,(x,r))\\le LR + P$ for $h\\in \\H_R$. Therefore our bound for $\\H_R$ is really\n",
    "$$\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|\\le\\frac{2\\sqrt{2}LR + (LR+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(2/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "The remaining variables $L$ and $P$ depend on the data distribution $D$, but not on $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a bogus argument: For every $h\\in \\H$, $h\\in \\H_{\\|h\\|}$. Therefore, with probability $1-\\delta$,\n",
    "$$\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|\\le\\frac{2\\sqrt{2}L\\|h\\| + (L\\|h\\|+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(2/\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "and so we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this argument is exactly the same as the problem with the Hoeffding's inequality argument at the beginning. Although this statement is true for each individual $h\\in \\H$, it is not true for all $h$ simultaneously. Just as with the Hoeffding's inequality argument, our medicine in this case will again be the union bound, but now with a twist. For the first time, we're going to make more careful use of the variable $\\delta$. Setting $\\delta_K=\\frac{6\\delta}{\\pi^2K^2}$, we have with probability at least $1-\\delta_K$, for all $h\\in \\H_{K}$, we have:\n",
    "$$\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|\\le\\frac{2\\sqrt{2}LK + (LK+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(2/\\delta_K)}}{\\sqrt{2N}}\n",
    "$$\n",
    "\n",
    "Put another way, the probability of the above equation NOT holding is at most $\\delta_K$. Now by union bound, the probability of th above equation holding for ALL $K=1,2,3,\\dots$ is at most $\\sum_{K=1}^\\infty\\delta_K = \\delta$. This is why we chose $\\delta_K$ in such a wacky manner - we're using the fact that $\\sum_{i=1}^\\infty \\frac{1}{i^2}=\\frac{\\pi^2}{6}$. \n",
    "\n",
    "In particular, we now have that with probability at least $1-\\delta$, for all $K$, for all $h\\in \\H_K$,\n",
    "$$\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|\\le\\frac{2\\sqrt{2}LK + (LK+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(\\pi^2K^2/3\\delta)}}{\\sqrt{2N}}\n",
    "$$\n",
    "Now observe that for any $h\\in \\H$, $h\\in H_{\\lceil\\|h\\|\\rceil}$, and $\\lceil\\|h\\|\\rceil\\le \\|h\\|+1$. Thus, the above equation means that with probability at least $1-\\delta$, for all $h\\in \\H$, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left|\\hat \\L(h) - \\L(h)\\right|&\\le\\frac{2\\sqrt{2}L(\\|h\\|+1) + (L(\\|h\\|+1)+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(\\pi^2(\\|h\\|+1)^2/3\\delta)}}{\\sqrt{2N}}\\\\\n",
    "&=O\\left(\\frac{(L\\|h\\|+L+P)\\sqrt{d\\log(N)+\\log(\\|h\\|/\\delta)+1}}{\\sqrt{N}}\\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Structual Risk Minimization and Regularization</h2>\n",
    "Now we've finally got a solid bound on our generalization performance. What does the bound tell us? One fact we can glean from this is that in order to half the generalization error we should probably roughly quadruple our amount of training data $N$. However, we can actually see something more exciting. One easy consequence of the bound is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\L(h)&\\le\\hat\\L(h)+\\frac{2\\sqrt{2}L(\\|h\\|+1) + (L(\\|h\\|+1)+P)\\sqrt{d\\log\\left(1+2\\sqrt{N}\\right)+\\log(\\pi^2(\\|h\\|+1)^2/3\\delta)}}{\\sqrt{2N}}\\\\\n",
    "&=\\hat \\L(h) + Q(h)\n",
    "\\end{align*}\n",
    "$$\n",
    "where we're defining $Q(h)$ to be the big complicated term in the left-hand-side of the above equation. One important observation is that we can actually compute $Q(h)$.\n",
    "This suggests that instead of finding $\\hat h$ that minimizes $\\hat \\L$ (which we called empirical risk minimization), it might be better to find a $\\hat h$ that minimizes $\\hat \\L(h) + Q(h)$. This is called <i>structural risk minimization</i>, and it's a good idea because doing this allows us to control our test error much more directly than we can by just minimizing $\\hat \\L$.\n",
    "\n",
    "However, if you have any experience with machine learning, you probably know that in practice no one whips out equations like the above when training a model. Instead, it is very common to introduce some kind of <i>weight decay</i>, or $L_2$ regularization. That is, it is common practice to find a $\\hat h$ that minimizes $\\hat \\L(h) + \\frac{\\lambda}{2}\\|h\\|^2$ for some user-specified value of $\\lambda$. It turns out that these are actually equivalent strategies! The justification for this can be a bit beyond what I want to cover here, but the underlying idea is that both $Q(h)$ and $\\frac{\\lambda^2}{2}\\|h\\|^2$ depend only on $\\|h\\|$ and so they have the same level-sets. Therefore by an argument with Lagrane Multipliers, one can show that there is some particular value of $\\lambda$ for which the two problems have the same solution.\n",
    "\n",
    "Just because using $\\frac{\\lambda}{2}\\|h\\|^2$ is equivalent to using $Q(h)$ for some value of $\\lambda$ isn't much of an argument for $\\frac{\\lambda}{2}\\|h\\|^2$ though - after all we still have to pick $\\lambda$ now! There are two more important reasons we use $L_2$ regularization instead of directly doing structural risk minimization. \n",
    "\n",
    "The first is that our value $Q(h)$ is extremely loose. Although the asymptotic dependence on $\\frac{1}{\\sqrt{N}}$ is correct, all of our constants are usually too big in practice. This is because we made many arguments that upper-bounded things, and at each step of the way we lost some constants that we don't know how to recover. In fact, there are much better bounds than the one I covered here. Using ideas like Rademacher Complexity or Replace-One-Stability, we can get rid of the $\\log(N)$ term as well as the dependence on dimension $d$. However, even with these improvements the bound often isn't tight enough. When our value for $Q$ is too loose, then $\\hat L(h)+Q(h)$ is dominated by $Q(h)$ and we end up just always setting $\\hat h\\approx 0$ since that minimizes $Q(h)$. However, we could imaging fixing this by using $\\lambda Q(h)$ for some user-specified $\\lambda$. The user could make use of a validation set to pick a $\\lambda$ that correctly counteracts the looseness in our value for $Q(h)$.\n",
    "\n",
    "This leads us to the second reason for using $\\frac{\\lambda}{2}\\|h\\|^2$, which is computational efficiency. Since the simplest way to fix the fact that $Q$ isn't tight is to use $\\lambda Q$ instead, we've lost the advantage of not having to tune a hyperparameter $\\lambda$, so that now the two methods really are equally good in terms of controlling overfitting. However, it turns out that using $L_2$ regularization often makes optimization algorithms much faster (because the objective now becomes strongly-convex, but that's another story)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What Was The Point?</h2>\n",
    "So if people don't even use this function $Q$ that we computed, why did we bother computing it? Well, one reason is just that I find it interesting. If that doesn't do it for you, then another is similar to the standard argument for big-Oh notation in algorithms analysis. The dependence of $O(1/\\sqrt{N})$ is roughly speaking correct, but our constants are too big. Even if our constants are off though, having the asymptotics right allows us to predict how much more data we'll need to improve our performance. Another is to help guide model selection. In general the term $LR\\sqrt{d}$ can be replaced by some measurement of <i>complexity</i> of the hypothesis class $\\H$, and so if you are choosing between two possible ways of modeling your data then you should favor the one with lower complexity.\n",
    "\n",
    "If this still seems weak to you, then perhaps it should be taken as a sign of work to be done! It would be great if we didn't have to resort to tuning some random $\\lambda$ hyperparameter, and perhaps with better analysis our constants would improve to the point that we wouldn't have to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Further Reading</h3>\n",
    "If you want to know more about generalization bounds, I recommend looking at the following\n",
    "<ol>\n",
    "<li><a href=http://jmlr.csail.mit.edu/papers/volume3/bartlett02a/bartlett02a.pdf>Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</a></li>\n",
    "<li><a href=http://www.stat.berkeley.edu/~bartlett/papers/bbm-lrc-02b.pdf>Local Rademacher Complexities</a></li>\n",
    "<li><a href=http://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf>Learnability, Stability, and Uniform Convergence</a></li>\n",
    "<li> Book: Probability in Banch Spaces by Ledoux and Talagrand</li>\n",
    "<li> Book: Statistical Learning Theory by Vapnik</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
